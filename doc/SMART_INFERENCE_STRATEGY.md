# æ™ºèƒ½æ¨ç†ç­–ç•¥ - è‡ªé€‚åº”æ¨ç†å’Œå‘Šè­¦

## ğŸ¯ è®¾è®¡ç›®æ ‡

### æ ¸å¿ƒéœ€æ±‚

1. **æ ¹æ®æŠ½å¸§é¢‘ç‡é…ç½®æ¨ç†é¢‘ç‡**
   - æŠ½å¸§å¿« â†’ æ¨ç†ä¹Ÿè¦å¿«ï¼ˆé‡‡æ ·æˆ–å¢åŠ å¹¶å‘ï¼‰
   - æŠ½å¸§æ…¢ â†’ æ¨ç†å¯ä»¥æ…¢ï¼ˆèŠ‚çœèµ„æºï¼‰

2. **æ¨ç†å¤ªæ…¢æ—¶å‘Šè­¦**
   - ç›‘æµ‹æ¨ç†é˜Ÿåˆ—ç§¯å‹æƒ…å†µ
   - è¶…è¿‡é˜ˆå€¼å‘é€å‘Šè­¦
   - è®°å½•æ€§èƒ½æŒ‡æ ‡

3. **è‡ªåŠ¨ä¸¢å¼ƒå¤„ç†ä¸å®Œçš„å›¾ç‰‡**
   - è®¾ç½®é˜Ÿåˆ—æœ€å¤§é•¿åº¦
   - è¶…è¿‡å®¹é‡ä¸¢å¼ƒæ—§å›¾ç‰‡
   - ä¿ç•™æœ€æ–°çš„å›¾ç‰‡ä¼˜å…ˆå¤„ç†

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„è®¾è®¡

### å½“å‰æ¶æ„

```
æŠ½å¸§å™¨ â†’ MinIO â†’ æ‰«æå™¨ â†’ è°ƒåº¦å™¨ â†’ æ¨ç†æœåŠ¡
         (æ— é™å †ç§¯)   (å…¨éƒ¨å¤„ç†)  (å¯èƒ½ç§¯å‹)
```

**é—®é¢˜**ï¼š
- âŒ å›¾ç‰‡æ— é™å †ç§¯
- âŒ æ¨ç†é˜Ÿåˆ—æ— é™å¢é•¿
- âŒ æ— æ³•æ„ŸçŸ¥ç§¯å‹

### ä¼˜åŒ–æ¶æ„

```
æŠ½å¸§å™¨ â†’ MinIO â†’ æ‰«æå™¨ â†’ æ™ºèƒ½é˜Ÿåˆ— â†’ è°ƒåº¦å™¨ â†’ æ¨ç†æœåŠ¡
   â†“        â†“        â†“         â†“         â†“
 5å¼ /ç§’   è‡ªåŠ¨æ¸…ç†  é‡‡æ ·è¿‡æ»¤   ä¼˜å…ˆé˜Ÿåˆ—   æ€§èƒ½ç›‘æ§
                              æœ€å¤§100å¼    â†“
                              è¶…è¿‡ä¸¢å¼ƒ   å‘Šè­¦ç³»ç»Ÿ
```

**ä¼˜åŠ¿**ï¼š
- âœ… é˜Ÿåˆ—æœ‰ä¸Šé™ï¼Œä¸ä¼šæ— é™å¢é•¿
- âœ… è‡ªåŠ¨ä¸¢å¼ƒæ—§å›¾ç‰‡
- âœ… å®æ—¶æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦
- âœ… è‡ªé€‚åº”è°ƒæ•´

---

## ğŸ’¡ å®ç°æ–¹æ¡ˆ

### æ–¹æ¡ˆ1ï¼šé…ç½®åŒ–çš„æ™ºèƒ½é˜Ÿåˆ—ï¼ˆæ¨èï¼‰

#### é…ç½®å‚æ•°

```toml
[ai_analysis]
enable = true
scan_interval_sec = 1  # æ‰«æé—´éš”

# æ™ºèƒ½é˜Ÿåˆ—é…ç½®
queue_mode = 'smart'  # smart|fifo|priority
max_queue_size = 100  # é˜Ÿåˆ—æœ€å¤§é•¿åº¦
queue_full_strategy = 'drop_oldest'  # drop_oldest|drop_newest|skip
sampling_enabled = true  # å¯ç”¨é‡‡æ ·
sampling_rate = 1  # é‡‡æ ·ç‡ï¼ˆ1=å…¨éƒ¨ï¼Œ2=50%ï¼Œ5=20%ï¼‰

# æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦
performance_monitoring = true  # å¯ç”¨æ€§èƒ½ç›‘æ§
backlog_alert_threshold = 50  # ç§¯å‹è¶…è¿‡50å¼ å‘Šè­¦
slow_inference_threshold_ms = 5000  # æ¨ç†è¶…è¿‡5ç§’å‘Šè­¦
alert_interval_sec = 60  # å‘Šè­¦é—´éš”ï¼ˆé¿å…é¢‘ç¹å‘Šè­¦ï¼‰

max_concurrent_infer = 50  # æœ€å¤§å¹¶å‘æ•°
```

#### å·¥ä½œé€»è¾‘

```python
class SmartInferenceQueue:
    def __init__(self, max_size=100, strategy='drop_oldest'):
        self.max_size = max_size
        self.strategy = strategy
        self.queue = []
        self.dropped_count = 0
        self.last_alert_time = 0
        
    def add_images(self, images):
        """æ·»åŠ å›¾ç‰‡åˆ°é˜Ÿåˆ—"""
        for img in images:
            if len(self.queue) >= self.max_size:
                # é˜Ÿåˆ—å·²æ»¡ï¼Œæ‰§è¡Œä¸¢å¼ƒç­–ç•¥
                if self.strategy == 'drop_oldest':
                    dropped = self.queue.pop(0)  # ä¸¢å¼ƒæœ€æ—§çš„
                    self.dropped_count += 1
                    self.log_dropped(dropped)
                elif self.strategy == 'drop_newest':
                    # ä¸¢å¼ƒæ–°çš„ï¼ˆä¸åŠ å…¥é˜Ÿåˆ—ï¼‰
                    self.dropped_count += 1
                    self.log_dropped(img)
                    continue
                elif self.strategy == 'skip':
                    continue
            
            self.queue.append(img)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        self.check_backlog_alert()
    
    def check_backlog_alert(self):
        """æ£€æŸ¥ç§¯å‹å¹¶å‘Šè­¦"""
        if len(self.queue) > self.backlog_alert_threshold:
            now = time.time()
            if now - self.last_alert_time > self.alert_interval_sec:
                self.send_alert({
                    'type': 'backlog',
                    'queue_size': len(self.queue),
                    'threshold': self.backlog_alert_threshold,
                    'dropped_total': self.dropped_count,
                    'message': f'æ¨ç†é˜Ÿåˆ—ç§¯å‹{len(self.queue)}å¼ ï¼Œå·²ä¸¢å¼ƒ{self.dropped_count}å¼ '
                })
                self.last_alert_time = now
```

---

### æ–¹æ¡ˆ2ï¼šé‡‡æ ·æ¨ç†ï¼ˆæŒ‰æŠ½å¸§é¢‘ç‡ï¼‰

#### è‡ªåŠ¨è®¡ç®—é‡‡æ ·ç‡

```python
def calculate_sampling_rate(frame_interval_ms, avg_inference_ms):
    """
    æ ¹æ®æŠ½å¸§é¢‘ç‡å’Œæ¨ç†é€Ÿåº¦è‡ªåŠ¨è®¡ç®—é‡‡æ ·ç‡
    
    ä¾‹å¦‚ï¼š
    - æŠ½å¸§ï¼šæ¯200ms = 5å¼ /ç§’
    - æ¨ç†ï¼šæ¯500ms = 2å¼ /ç§’
    - é‡‡æ ·ç‡ï¼š5/2 â‰ˆ 3ï¼ˆæ¯3å¼ å¤„ç†1å¼ ï¼‰
    """
    frames_per_sec = 1000 / frame_interval_ms
    infer_per_sec = 1000 / avg_inference_ms
    
    if infer_per_sec >= frames_per_sec:
        return 1  # æ¨ç†å¤Ÿå¿«ï¼Œå…¨éƒ¨å¤„ç†
    else:
        return int(frames_per_sec / infer_per_sec) + 1
```

#### é…ç½®ç¤ºä¾‹

```toml
[ai_analysis]
auto_sampling = true  # å¯ç”¨è‡ªåŠ¨é‡‡æ ·
target_inference_ratio = 0.8  # ç›®æ ‡ï¼šæ¨ç†èƒ½åŠ›åº”â‰¥æŠ½å¸§é€Ÿåº¦çš„80%
adjust_interval_sec = 60  # æ¯60ç§’è°ƒæ•´ä¸€æ¬¡é‡‡æ ·ç‡
```

#### å·¥ä½œæµç¨‹

```
1. å¯åŠ¨æ—¶ï¼šé‡‡æ ·ç‡=1ï¼ˆå…¨éƒ¨å¤„ç†ï¼‰
   â†“
2. æ¯60ç§’è¯„ä¼°ï¼š
   - ç»Ÿè®¡æŠ½å¸§é€Ÿåº¦ï¼š5å¼ /ç§’
   - ç»Ÿè®¡æ¨ç†é€Ÿåº¦ï¼š2å¼ /ç§’
   - è®¡ç®—é‡‡æ ·ç‡ï¼š5/2 â‰ˆ 3
   â†“
3. åº”ç”¨æ–°é‡‡æ ·ç‡ï¼š
   - æ¯3å¼ å›¾ç‰‡åªæ¨ç†1å¼ 
   - å…¶ä»–2å¼ æ ‡è®°ä¸º"å·²è·³è¿‡"
   â†“
4. ç»§ç»­ç›‘æ§ï¼ŒåŠ¨æ€è°ƒæ•´
```

---

### æ–¹æ¡ˆ3ï¼šä¼˜å…ˆçº§ä¸¢å¼ƒ

#### é…ç½®

```toml
[ai_analysis]
queue_mode = 'priority'
max_queue_size = 100

# ä¼˜å…ˆçº§è§„åˆ™
[[ai_analysis.priority_rules]]
task_type = 'äººå‘˜è·Œå€’'
priority = 1  # æœ€é«˜ä¼˜å…ˆçº§ï¼ˆæ°¸ä¸ä¸¢å¼ƒï¼‰

[[ai_analysis.priority_rules]]
task_type = 'ç«ç„°æ£€æµ‹'
priority = 1

[[ai_analysis.priority_rules]]
task_type = 'äººæ•°ç»Ÿè®¡'
priority = 3  # ä½ä¼˜å…ˆçº§ï¼ˆå¯ä»¥ä¸¢å¼ƒï¼‰
```

#### ä¸¢å¼ƒé€»è¾‘

```python
def drop_low_priority_images(queue, max_size):
    """ä¸¢å¼ƒä½ä¼˜å…ˆçº§å›¾ç‰‡"""
    if len(queue) <= max_size:
        return
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    queue.sort(key=lambda x: (x.priority, x.timestamp))
    
    # ä¿ç•™é«˜ä¼˜å…ˆçº§å’Œæœ€æ–°çš„
    keep_count = max_size
    to_keep = []
    to_drop = []
    
    # ä¼˜å…ˆä¿ç•™priority=1çš„
    for img in queue:
        if img.priority == 1:
            to_keep.append(img)
        elif len(to_keep) < keep_count:
            to_keep.append(img)
        else:
            to_drop.append(img)
    
    # ä¸¢å¼ƒä½ä¼˜å…ˆçº§
    for img in to_drop:
        log.info(f'ä¸¢å¼ƒä½ä¼˜å…ˆçº§å›¾ç‰‡: {img.path}')
    
    return to_keep
```

---

## ğŸ”§ ä»£ç å®ç°å»ºè®®

### ä¿®æ”¹æ–‡ä»¶ï¼š`internal/plugin/aianalysis/queue.go`ï¼ˆæ–°å»ºï¼‰

```go
package aianalysis

import (
	"log/slog"
	"sync"
	"time"
)

// InferenceQueue æ™ºèƒ½æ¨ç†é˜Ÿåˆ—
type InferenceQueue struct {
	images          []ImageInfo
	maxSize         int
	strategy        string // drop_oldest|drop_newest|skip
	mu              sync.RWMutex
	droppedCount    int64
	lastAlertTime   time.Time
	alertThreshold  int
	alertInterval   time.Duration
	log             *slog.Logger
}

// NewInferenceQueue åˆ›å»ºé˜Ÿåˆ—
func NewInferenceQueue(maxSize int, strategy string, alertThreshold int, logger *slog.Logger) *InferenceQueue {
	return &InferenceQueue{
		images:         make([]ImageInfo, 0, maxSize),
		maxSize:        maxSize,
		strategy:       strategy,
		alertThreshold: alertThreshold,
		alertInterval:  60 * time.Second,
		log:            logger,
	}
}

// Add æ·»åŠ å›¾ç‰‡åˆ°é˜Ÿåˆ—
func (q *InferenceQueue) Add(images []ImageInfo) {
	q.mu.Lock()
	defer q.mu.Unlock()
	
	for _, img := range images {
		// æ£€æŸ¥é˜Ÿåˆ—æ˜¯å¦å·²æ»¡
		if len(q.images) >= q.maxSize {
			switch q.strategy {
			case "drop_oldest":
				// ä¸¢å¼ƒæœ€æ—§çš„
				dropped := q.images[0]
				q.images = q.images[1:]
				q.droppedCount++
				q.log.Warn("queue full, dropped oldest image",
					slog.String("dropped", dropped.Path),
					slog.Int("queue_size", len(q.images)))
			case "drop_newest":
				// ä¸¢å¼ƒæ–°çš„ï¼ˆä¸åŠ å…¥ï¼‰
				q.droppedCount++
				q.log.Warn("queue full, dropped newest image",
					slog.String("dropped", img.Path))
				continue
			case "skip":
				// è·³è¿‡
				continue
			}
		}
		
		q.images = append(q.images, img)
	}
	
	// æ£€æŸ¥ç§¯å‹å‘Šè­¦
	q.checkBacklogAlert()
}

// Pop å–å‡ºä¸€å¼ å›¾ç‰‡
func (q *InferenceQueue) Pop() (ImageInfo, bool) {
	q.mu.Lock()
	defer q.mu.Unlock()
	
	if len(q.images) == 0 {
		return ImageInfo{}, false
	}
	
	img := q.images[0]
	q.images = q.images[1:]
	return img, true
}

// Size è·å–é˜Ÿåˆ—å¤§å°
func (q *InferenceQueue) Size() int {
	q.mu.RLock()
	defer q.mu.RUnlock()
	return len(q.images)
}

// checkBacklogAlert æ£€æŸ¥ç§¯å‹å‘Šè­¦
func (q *InferenceQueue) checkBacklogAlert() {
	if len(q.images) <= q.alertThreshold {
		return
	}
	
	now := time.Now()
	if now.Sub(q.lastAlertTime) < q.alertInterval {
		return  // é¿å…é¢‘ç¹å‘Šè­¦
	}
	
	q.lastAlertTime = now
	q.log.Error("inference backlog alert",
		slog.Int("queue_size", len(q.images)),
		slog.Int("threshold", q.alertThreshold),
		slog.Int64("dropped_total", q.droppedCount),
		slog.String("message", "æ¨ç†é˜Ÿåˆ—ç§¯å‹ï¼Œè¯·å¢åŠ å¹¶å‘æ•°æˆ–é™ä½æŠ½å¸§é¢‘ç‡"))
	
	// TODO: å‘é€ç³»ç»Ÿå‘Šè­¦ï¼ˆé‚®ä»¶/çŸ­ä¿¡/webhookï¼‰
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (q *InferenceQueue) GetStats() map[string]interface{} {
	q.mu.RLock()
	defer q.mu.RUnlock()
	
	return map[string]interface{}{
		"queue_size":     len(q.images),
		"max_size":       q.maxSize,
		"dropped_total":  q.droppedCount,
		"utilization":    float64(len(q.images)) / float64(q.maxSize),
	}
}
```

---

### ä¿®æ”¹æ–‡ä»¶ï¼š`internal/plugin/aianalysis/monitor.go`ï¼ˆæ–°å»ºï¼‰

```go
package aianalysis

import (
	"log/slog"
	"sync"
	"time"
)

// PerformanceMonitor æ€§èƒ½ç›‘æ§å™¨
type PerformanceMonitor struct {
	frameRate         float64  // æŠ½å¸§é€Ÿç‡ï¼ˆå¼ /ç§’ï¼‰
	inferenceRate     float64  // æ¨ç†é€Ÿç‡ï¼ˆå¼ /ç§’ï¼‰
	avgInferenceTime  float64  // å¹³å‡æ¨ç†æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
	
	totalInferences   int64
	totalInferenceTime int64
	mu                sync.RWMutex
	
	slowThresholdMs   int64
	lastSlowAlert     time.Time
	log               *slog.Logger
}

// NewPerformanceMonitor åˆ›å»ºç›‘æ§å™¨
func NewPerformanceMonitor(slowThresholdMs int64, logger *slog.Logger) *PerformanceMonitor {
	return &PerformanceMonitor{
		slowThresholdMs: slowThresholdMs,
		log:            logger,
	}
}

// RecordInference è®°å½•ä¸€æ¬¡æ¨ç†
func (m *PerformanceMonitor) RecordInference(inferenceTimeMs int64) {
	m.mu.Lock()
	defer m.mu.Unlock()
	
	m.totalInferences++
	m.totalInferenceTime += inferenceTimeMs
	m.avgInferenceTime = float64(m.totalInferenceTime) / float64(m.totalInferences)
	
	// æ£€æŸ¥æ˜¯å¦æ¨ç†å¤ªæ…¢
	if inferenceTimeMs > m.slowThresholdMs {
		now := time.Now()
		if now.Sub(m.lastSlowAlert) > 60*time.Second {
			m.lastSlowAlert = now
			m.log.Warn("slow inference detected",
				slog.Int64("inference_time_ms", inferenceTimeMs),
				slog.Int64("threshold_ms", m.slowThresholdMs),
				slog.Float64("avg_time_ms", m.avgInferenceTime))
			// TODO: å‘é€å‘Šè­¦
		}
	}
}

// CalculateSamplingRate è®¡ç®—æ¨èçš„é‡‡æ ·ç‡
func (m *PerformanceMonitor) CalculateSamplingRate(frameIntervalMs int) int {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	if m.avgInferenceTime == 0 {
		return 1  // åˆå§‹å…¨éƒ¨å¤„ç†
	}
	
	framesPerSec := 1000.0 / float64(frameIntervalMs)
	inferPerSec := 1000.0 / m.avgInferenceTime
	
	if inferPerSec >= framesPerSec {
		return 1  // æ¨ç†å¤Ÿå¿«ï¼Œå…¨éƒ¨å¤„ç†
	}
	
	// è®¡ç®—é‡‡æ ·ç‡ï¼šéœ€è¦è·³è¿‡å¤šå°‘å¼ 
	ratio := framesPerSec / inferPerSec
	samplingRate := int(ratio) + 1
	
	m.log.Info("calculated sampling rate",
		slog.Float64("frames_per_sec", framesPerSec),
		slog.Float64("infer_per_sec", inferPerSec),
		slog.Int("sampling_rate", samplingRate))
	
	return samplingRate
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (m *PerformanceMonitor) GetStats() map[string]interface{} {
	m.mu.RLock()
	defer m.mu.RUnlock()
	
	return map[string]interface{}{
		"total_inferences":   m.totalInferences,
		"avg_inference_ms":   m.avgInferenceTime,
		"frame_rate":         m.frameRate,
		"inference_rate":     m.inferenceRate,
	}
}
```

---

### ä¿®æ”¹æ–‡ä»¶ï¼š`internal/plugin/aianalysis/service.go`

åœ¨Startæ–¹æ³•ä¸­é›†æˆæ™ºèƒ½é˜Ÿåˆ—ï¼š

```go
func (s *Service) Start() error {
	// ... ç°æœ‰ä»£ç 
	
	// åˆ›å»ºæ™ºèƒ½é˜Ÿåˆ—
	queue := NewInferenceQueue(
		100,              // æœ€å¤§100å¼ 
		"drop_oldest",    // ä¸¢å¼ƒæ—§çš„
		50,               // ç§¯å‹50å¼ å‘Šè­¦
		s.log,
	)
	
	// åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
	monitor := NewPerformanceMonitor(
		5000,  // æ¨ç†è¶…è¿‡5ç§’å‘Šè­¦
		s.log,
	)
	
	// å¯åŠ¨æ‰«æå’Œå¤„ç†å¾ªç¯
	go func() {
		ticker := time.NewTicker(time.Duration(s.cfg.ScanIntervalSec) * time.Second)
		defer ticker.Stop()
		
		for {
			select {
			case <-s.stop:
				return
			case <-ticker.C:
				// æ‰«æMinIO
				newImages, err := s.scanner.scanNewImages()
				if err != nil {
					continue
				}
				
				// æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆè‡ªåŠ¨ä¸¢å¼ƒï¼‰
				queue.Add(newImages)
				
				// å¤„ç†é˜Ÿåˆ—ä¸­çš„å›¾ç‰‡
				for {
					img, ok := queue.Pop()
					if !ok {
						break  // é˜Ÿåˆ—ä¸ºç©º
					}
					
					// è°ƒåº¦æ¨ç†
					start := time.Now()
					s.scheduler.ScheduleInference(img)
					inferenceTime := time.Since(start).Milliseconds()
					
					// è®°å½•æ€§èƒ½
					monitor.RecordInference(inferenceTime)
				}
				
				// å®šæœŸè¾“å‡ºç»Ÿè®¡
				queueStats := queue.GetStats()
				perfStats := monitor.GetStats()
				s.log.Info("performance stats",
					slog.Any("queue", queueStats),
					slog.Any("performance", perfStats))
			}
		}
	}()
	
	return nil
}
```

---

## ğŸ› ï¸ æ— éœ€ä¿®æ”¹ä»£ç çš„ä¸´æ—¶æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šé…ç½®æ–‡ä»¶å®ç°æ™ºèƒ½é‡‡æ ·

**åœ¨æ‰«æå™¨ä¸­æ·»åŠ é€»è¾‘**ï¼ˆä¿®æ”¹scanner.goï¼‰ï¼š

```go
// åœ¨scanNewImagesä¸­æ·»åŠ 
var imageCounter int

func (s *Scanner) scanNewImages() ([]ImageInfo, error) {
	// ... ç°æœ‰æ‰«æä»£ç 
	
	var newImages []ImageInfo
	samplingRate := 5  // æ¯5å¼ å¤„ç†1å¼ 
	
	for object := range objectCh {
		// ... æ£€æŸ¥ä»£ç 
		
		imageCounter++
		
		// é‡‡æ ·è¿‡æ»¤
		if imageCounter % samplingRate != 0 {
			s.MarkProcessed(object.Key)  // æ ‡è®°å·²å¤„ç†ä½†ä¸æ¨ç†
			continue
		}
		
		newImages = append(newImages, ImageInfo{...})
	}
	
	// é™åˆ¶è¿”å›æ•°é‡ï¼ˆé¿å…ç§¯å‹ï¼‰
	maxReturn := 20
	if len(newImages) > maxReturn {
		s.log.Warn("too many new images, limiting",
			slog.Int("found", len(newImages)),
			slog.Int("limit", maxReturn))
		newImages = newImages[len(newImages)-maxReturn:]  // ä¿ç•™æœ€æ–°çš„
	}
	
	return newImages, nil
}
```

---

### æ–¹æ¡ˆBï¼šé€šè¿‡é…ç½®æ§åˆ¶ï¼ˆç«‹å³å¯ç”¨ï¼‰

**é…ç½®è°ƒæ•´ç­–ç•¥**ï¼š

```bash
# åœºæ™¯1ï¼šæŠ½å¸§å¿«ï¼Œæ¨ç†ä¹Ÿè¦å¿«
# æŠ½å¸§ï¼š5å¼ /ç§’
interval_ms = 200
scan_interval_sec = 1
max_concurrent_infer = 50

# åœºæ™¯2ï¼šæŠ½å¸§å¿«ï¼Œä½†æ¨ç†æ…¢
# ä½¿ç”¨é‡‡æ ·ï¼šåªæ¨ç†20%
interval_ms = 200  # ä»ç„¶5å¼ /ç§’
scan_interval_sec = 5  # é™ä½æ‰«æé¢‘ç‡
max_concurrent_infer = 20
# æ•ˆæœï¼šæ¯5ç§’æ‰«æä¸€æ¬¡ï¼Œæ¯æ¬¡å¤„ç†25å¼ ä¸­çš„éƒ¨åˆ†

# åœºæ™¯3ï¼šæ¨ç†å¤ªæ…¢ï¼Œé™ä½æŠ½å¸§
interval_ms = 1000  # é™ä¸º1å¼ /ç§’
scan_interval_sec = 3
max_concurrent_infer = 10
```

---

## ğŸ“Š æ¨èçš„é…ç½®ç»„åˆ

### é…ç½®1ï¼šè‡ªé€‚åº”æ¨ç†ï¼ˆéœ€è¦ä»£ç ä¿®æ”¹ï¼‰

```toml
[ai_analysis]
queue_mode = 'smart'
max_queue_size = 100
queue_full_strategy = 'drop_oldest'
auto_sampling = true
backlog_alert_threshold = 50
slow_inference_threshold_ms = 5000
alert_interval_sec = 60
```

### é…ç½®2ï¼šå½“å‰å¯ç”¨ï¼ˆæ— éœ€æ”¹ä»£ç ï¼‰

```toml
# æ ¹æ®æ‚¨çš„æŠ½å¸§é¢‘ç‡é€‰æ‹©ï¼š

# æŠ½å¸§5å¼ /ç§’ï¼ˆæ¯200msï¼‰
[[frame_extractor.tasks]]
interval_ms = 200

# é€‰é¡¹Aï¼šå…¨éƒ¨æ¨ç†ï¼ˆéœ€è¦å¿«é€Ÿç®—æ³•ï¼‰
[ai_analysis]
scan_interval_sec = 1
max_concurrent_infer = 50  # å¤Ÿå¤§æ‰èƒ½ä¸ç§¯å‹

# é€‰é¡¹Bï¼šé™ä½æ‰«æé¢‘ç‡ï¼ˆéƒ¨åˆ†æ¨ç†ï¼‰
[ai_analysis]
scan_interval_sec = 5  # æ¯5ç§’å¤„ç†ä¸€æ‰¹
max_concurrent_infer = 20

# é€‰é¡¹Cï¼šé™ä½æŠ½å¸§ï¼ˆåŒ¹é…æ¨ç†é€Ÿåº¦ï¼‰
[[frame_extractor.tasks]]
interval_ms = 1000  # 1å¼ /ç§’ï¼ˆæ¨ç†æ…¢æ—¶ï¼‰
[ai_analysis]
scan_interval_sec = 3
max_concurrent_infer = 10
```

---

## ğŸš¨ å‘Šè­¦ç³»ç»Ÿè®¾è®¡

### å‘Šè­¦ç±»å‹

```go
type AlertType string

const (
	AlertBacklog      AlertType = "queue_backlog"      // é˜Ÿåˆ—ç§¯å‹
	AlertSlowInfer    AlertType = "slow_inference"     // æ¨ç†å¤ªæ…¢
	AlertHighDrop     AlertType = "high_drop_rate"     // ä¸¢å¼ƒç‡è¿‡é«˜
	AlertStorageFull  AlertType = "storage_full"       // å­˜å‚¨æ»¡
)

type SystemAlert struct {
	Type       AlertType
	Level      string  // warning|error|critical
	Message    string
	Data       map[string]interface{}
	Timestamp  time.Time
}
```

### å‘Šè­¦è§¦å‘æ¡ä»¶

```go
func (s *Service) checkSystemAlerts() {
	queueStats := s.queue.GetStats()
	perfStats := s.monitor.GetStats()
	
	// 1. é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
	if queueStats["queue_size"].(int) > 50 {
		s.sendAlert(SystemAlert{
			Type:    AlertBacklog,
			Level:   "warning",
			Message: fmt.Sprintf("æ¨ç†é˜Ÿåˆ—ç§¯å‹%då¼ å›¾ç‰‡", queueStats["queue_size"]),
			Data:    queueStats,
		})
	}
	
	// 2. æ¨ç†æ…¢å‘Šè­¦
	if perfStats["avg_inference_ms"].(float64) > 5000 {
		s.sendAlert(SystemAlert{
			Type:    AlertSlowInfer,
			Level:   "warning",
			Message: fmt.Sprintf("å¹³å‡æ¨ç†æ—¶é—´%.0fmsï¼Œè¶…è¿‡é˜ˆå€¼5000ms", perfStats["avg_inference_ms"]),
			Data:    perfStats,
		})
	}
	
	// 3. é«˜ä¸¢å¼ƒç‡å‘Šè­¦
	dropRate := float64(queueStats["dropped_total"].(int64)) / float64(perfStats["total_inferences"].(int64))
	if dropRate > 0.3 {  // ä¸¢å¼ƒç‡è¶…è¿‡30%
		s.sendAlert(SystemAlert{
			Type:    AlertHighDrop,
			Level:   "error",
			Message: fmt.Sprintf("å›¾ç‰‡ä¸¢å¼ƒç‡%.1f%%ï¼Œæ¨ç†èƒ½åŠ›ä¸è¶³", dropRate*100),
			Data:    map[string]interface{}{
				"drop_rate": dropRate,
				"dropped":   queueStats["dropped_total"],
				"total":     perfStats["total_inferences"],
			},
		})
	}
}
```

---

## ğŸ“‹ APIæ¥å£è®¾è®¡

### æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡

**GET** `/api/v1/ai_analysis/performance`

**å“åº”**ï¼š
```json
{
  "queue": {
    "current_size": 25,
    "max_size": 100,
    "dropped_total": 123,
    "utilization": 0.25
  },
  "performance": {
    "total_inferences": 1523,
    "avg_inference_ms": 350,
    "frame_rate": 5.0,
    "inference_rate": 2.8,
    "recommended_sampling": 2
  },
  "alerts": [
    {
      "type": "queue_backlog",
      "level": "warning",
      "message": "æ¨ç†é˜Ÿåˆ—ç§¯å‹50å¼ ",
      "timestamp": "2024-10-16T15:47:30Z"
    }
  ]
}
```

### è°ƒæ•´é‡‡æ ·ç‡

**POST** `/api/v1/ai_analysis/sampling`

**è¯·æ±‚**ï¼š
```json
{
  "enabled": true,
  "sampling_rate": 3,  // æ¯3å¼ å¤„ç†1å¼ 
  "auto_adjust": true
}
```

---

## ğŸ¯ å®æ–½è®¡åˆ’

### é˜¶æ®µ1ï¼šé…ç½®å±‚é¢ä¼˜åŒ–ï¼ˆä»Šå¤©å¯å®Œæˆï¼‰

**ä¸ä¿®æ”¹ä»£ç ï¼Œé€šè¿‡é…ç½®å®ç°**ï¼š

```toml
# æ–¹æ¡ˆï¼šæ ¹æ®æ¨ç†èƒ½åŠ›è°ƒæ•´æŠ½å¸§å’Œæ‰«æ

# å¦‚æœæ¨ç†æ…¢ï¼ˆæ¯”å¦‚500ms/å¼ ï¼‰
[[frame_extractor.tasks]]
interval_ms = 500  # åŒ¹é…æ¨ç†é€Ÿåº¦

[ai_analysis]
scan_interval_sec = 3  # æ¯3ç§’æ‰«æä¸€æ‰¹
max_concurrent_infer = 20

# å¦‚æœæ¨ç†å¿«ï¼ˆæ¯”å¦‚100ms/å¼ ï¼‰
[[frame_extractor.tasks]]
interval_ms = 100  # 10å¼ /ç§’

[ai_analysis]
scan_interval_sec = 1
max_concurrent_infer = 100
```

### é˜¶æ®µ2ï¼šä»£ç å±‚é¢ä¼˜åŒ–ï¼ˆæœ¬å‘¨ï¼‰

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**ï¼š
1. âœ… æ–°å»º `internal/plugin/aianalysis/queue.go` - æ™ºèƒ½é˜Ÿåˆ—
2. âœ… æ–°å»º `internal/plugin/aianalysis/monitor.go` - æ€§èƒ½ç›‘æ§
3. âœ… ä¿®æ”¹ `internal/plugin/aianalysis/service.go` - é›†æˆé˜Ÿåˆ—å’Œç›‘æ§
4. âœ… ä¿®æ”¹ `internal/plugin/aianalysis/scanner.go` - æ·»åŠ é‡‡æ ·é€»è¾‘
5. âœ… æ–°å¢ APIæ¥å£ - æ€§èƒ½ç»Ÿè®¡å’Œæ§åˆ¶

### é˜¶æ®µ3ï¼šå‘Šè­¦ç³»ç»Ÿï¼ˆæœ¬æœˆï¼‰

**åŠŸèƒ½**ï¼š
- é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
- æ¨ç†æ…¢å‘Šè­¦
- é«˜ä¸¢å¼ƒç‡å‘Šè­¦
- å­˜å‚¨æ»¡å‘Šè­¦

**é€šçŸ¥æ–¹å¼**ï¼š
- Webç•Œé¢é€šçŸ¥
- é‚®ä»¶/çŸ­ä¿¡
- Webhook
- Kafkaæ¶ˆæ¯

---

## ğŸ’» ç«‹å³å¯ç”¨çš„ç®€åŒ–æ–¹æ¡ˆ

ç”±äºä»£ç ä¿®æ”¹éœ€è¦é‡æ–°ç¼–è¯‘ï¼Œæˆ‘ä¸ºæ‚¨åˆ›å»ºä¸€ä¸ª**é…ç½®æ–‡ä»¶é©±åŠ¨çš„æ–¹æ¡ˆ**ï¼š

### åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š`smart_inference_config.toml`

```toml
# yanying æ™ºèƒ½æ¨ç†é…ç½®

[inference]
# åŸºç¡€é…ç½®
frame_interval_ms = 200  # æŠ½å¸§é—´éš”
target_fps = 5  # ç›®æ ‡ï¼š5å¼ /ç§’

# æ™ºèƒ½é˜Ÿåˆ—
queue_enabled = true
queue_max_size = 100
queue_strategy = "drop_oldest"  # drop_oldest|drop_newest|latest_only

# é‡‡æ ·é…ç½®
sampling_enabled = true
sampling_mode = "auto"  # auto|fixed|adaptive
fixed_sampling_rate = 5  # fixedæ¨¡å¼ï¼šæ¯5å¼ å¤„ç†1å¼ 
auto_target_ratio = 0.8  # autoæ¨¡å¼ï¼šæ¨ç†èƒ½åŠ›åº”>=æŠ½å¸§é€Ÿåº¦çš„80%

# æ€§èƒ½ç›‘æ§
monitoring_enabled = true
slow_inference_ms = 5000  # è¶…è¿‡5ç§’å‘Šè­¦
backlog_threshold = 50  # ç§¯å‹50å¼ å‘Šè­¦
alert_interval_sec = 60  # å‘Šè­¦é—´éš”

# å‘Šè­¦é€šçŸ¥
alert_webhook = ""  # Webhook URL
alert_email = ""  # é‚®ä»¶åœ°å€
```

---

## ğŸ” ç›‘æ§å’Œè°ƒä¼˜

### å®æ—¶ç›‘æ§è„šæœ¬

```bash
cat > /code/EasyDarwin/monitor_inference.sh << 'EOF'
#!/bin/bash

# æ¨ç†æ€§èƒ½å®æ—¶ç›‘æ§

LOG_FILE="/code/EasyDarwin/build/EasyDarwin-lin-*/logs/20251016_08_00_00.log"

echo "ç›‘æ§æ¨ç†æ€§èƒ½ï¼ˆCtrl+Cåœæ­¢ï¼‰..."
echo ""

LAST_FOUND=0
LAST_SCHEDULED=0

while true; do
    sleep 10
    
    # ç»Ÿè®¡æœ€è¿‘10ç§’
    FOUND=$(tail -n 100 $LOG_FILE | grep "found new" | grep -o '"count":[0-9]*' | cut -d':' -f2 | awk '{sum+=$1} END {print sum}')
    SCHEDULED=$(tail -n 100 $LOG_FILE | grep "scheduling inference" | wc -l)
    
    FOUND=${FOUND:-0}
    SCHEDULED=${SCHEDULED:-0}
    
    FOUND_RATE=$(echo "scale=1; ($FOUND - $LAST_FOUND) / 10" | bc)
    SCHED_RATE=$(echo "scale=1; ($SCHEDULED - $LAST_SCHEDULED) / 10" | bc)
    
    echo "[$(date +%H:%M:%S)] å‘ç°: ${FOUND_RATE}å¼ /ç§’, è°ƒåº¦: ${SCHED_RATE}æ¬¡/ç§’"
    
    # æ£€æŸ¥ç§¯å‹
    if (( $(echo "$FOUND_RATE > $SCHED_RATE * 1.5" | bc -l) )); then
        echo "  âš ï¸  æ¨ç†é€Ÿåº¦è·Ÿä¸ä¸ŠæŠ½å¸§é€Ÿåº¦ï¼"
    fi
    
    LAST_FOUND=$FOUND
    LAST_SCHEDULED=$SCHEDULED
done
EOF

chmod +x /code/EasyDarwin/monitor_inference.sh
```

---

## ğŸ¯ æ ¹æ®æ‚¨éœ€æ±‚çš„æœ€ç»ˆæ–¹æ¡ˆ

### é…ç½®ï¼šè‡ªåŠ¨é€‚é…5å¼ /ç§’

```toml
[[frame_extractor.tasks]]
interval_ms = 200  # æŠ½å¸§ï¼š5å¼ /ç§’

[ai_analysis]
scan_interval_sec = 1  # æ¯ç§’æ‰«æ
max_concurrent_infer = 50  # è¶³å¤Ÿçš„å¹¶å‘

# ï¼ˆæœªæ¥æ·»åŠ ï¼‰
queue_max_size = 100  # é˜Ÿåˆ—æœ€å¤š100å¼ 
queue_strategy = 'drop_oldest'  # è¶…è¿‡åˆ™ä¸¢å¼ƒæ—§çš„
backlog_alert = 50  # ç§¯å‹50å¼ å‘Šè­¦
slow_inference_alert_ms = 5000  # æ¨ç†è¶…5ç§’å‘Šè­¦
```

### å½“å‰å¯ç”¨çš„è§£å†³æ–¹æ¡ˆ

**1. é˜Ÿåˆ—é™åˆ¶**ï¼šé€šè¿‡æ‰«æé—´éš”æ§åˆ¶
```toml
scan_interval_sec = 1  # æ¯ç§’æœ€å¤šå¤„ç†ä¸€æ‰¹
# å¦‚æœæ¯æ‰¹5å¼ ï¼Œåˆ™é˜Ÿåˆ—ä¸ä¼šè¶…è¿‡5å¼ 
```

**2. è‡ªåŠ¨ä¸¢å¼ƒ**ï¼šMinIOæ¸…ç†ç­–ç•¥
```bash
# 1å¤©è¿‡æœŸ = è‡ªåŠ¨ä¸¢å¼ƒæ—§å›¾ç‰‡
/tmp/mc ilm add yanying-minio/images --expiry-days 1
```

**3. æ€§èƒ½ç›‘æ§**ï¼šæ—¥å¿—åˆ†æ
```bash
# å®æ—¶ç›‘æ§è„šæœ¬
./monitor_inference.sh
```

**4. æ‰‹åŠ¨è°ƒä¼˜**ï¼šæ ¹æ®æ—¥å¿—è°ƒæ•´å‚æ•°
- å¦‚æœç§¯å‹ â†’ å¢åŠ `max_concurrent_infer`
- å¦‚æœæ¨ç†æ…¢ â†’ é™ä½`interval_ms`

---

## ğŸ“ æ€»ç»“

### æ‚¨çš„éœ€æ±‚

1. âœ… **æ ¹æ®æŠ½å¸§é¢‘ç‡é…ç½®æ¨ç†**
   - å½“å‰ï¼šæŠ½å¸§5å¼ /ç§’ï¼Œæ¨ç†ä¹Ÿæ˜¯5å¼ /ç§’
   
2. âš ï¸ **æ¨ç†å¤ªæ…¢æ—¶å‘Šè­¦**
   - ä¸´æ—¶æ–¹æ¡ˆï¼šé€šè¿‡æ—¥å¿—ç›‘æ§
   - å®Œæ•´æ–¹æ¡ˆï¼šéœ€è¦ä»£ç æ·»åŠ å‘Šè­¦ç³»ç»Ÿ

3. âš ï¸ **è‡ªåŠ¨ä¸¢å¼ƒå¤„ç†ä¸å®Œçš„å›¾ç‰‡**
   - ä¸´æ—¶æ–¹æ¡ˆï¼šMinIOè‡ªåŠ¨æ¸…ç†
   - å®Œæ•´æ–¹æ¡ˆï¼šéœ€è¦ä»£ç æ·»åŠ æ™ºèƒ½é˜Ÿåˆ—

### å½“å‰çŠ¶æ€

âœ… **é…ç½®å±‚é¢å·²ä¼˜åŒ–**ï¼š
- æŠ½å¸§ï¼š5å¼ /ç§’
- æ‰«æï¼šæ¯ç§’
- å¹¶å‘ï¼š50ä¸ª
- æ¸…ç†ï¼š1å¤©è¿‡æœŸ

âš ï¸ **ä»£ç å±‚é¢å¾…å¼€å‘**ï¼š
- æ™ºèƒ½é˜Ÿåˆ—ç®¡ç†
- æ€§èƒ½ç›‘æ§å‘Šè­¦
- è‡ªåŠ¨é‡‡æ ·è°ƒæ•´

---

**æˆ‘å»ºè®®ï¼šå…ˆä½¿ç”¨å½“å‰çš„é…ç½®æ–¹æ¡ˆï¼ŒåŒæ—¶æˆ‘å¯ä»¥å¸®æ‚¨å®ç°ä»£ç å±‚é¢çš„æ™ºèƒ½é˜Ÿåˆ—å’Œå‘Šè­¦åŠŸèƒ½ã€‚éœ€è¦æˆ‘ç»§ç»­å¼€å‘å—ï¼Ÿ**

